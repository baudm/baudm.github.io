<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Democratizing autonomous driving research and development | Darwin M. Bautista </title> <meta name="author" content="Darwin M. Bautista"> <meta name="description" content="Largely based on Dr. Holger Caesar's research talk at UP EEEI"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://baudm.github.io/blog/2024/datasets-for-autonomous-vehicles/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">figure:has(iframe){position:relative;overflow:hidden;width:100%;padding-top:56.25%}.responsive-iframe{position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Democratizing autonomous driving research and development",
            "description": "Largely based on Dr. Holger Caesar's research talk at UP EEEI",
            "published": "May 17, 2024",
            "authors": [
              
              {
                "author": "Darwin Bautista",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Samsung R&D Institute Philippines",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Darwin</span> M. Bautista </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Democratizing autonomous driving research and development</h1> <p>Largely based on Dr. Holger Caesar's research talk at UP EEEI</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-case-for-autonomous-vehicles">The case for autonomous vehicles</a> </div> <div> <a href="#road-to-full-autonomy">Road to full autonomy</a> </div> <div> <a href="#towards-affordable-large-scale-autonomous-driving-datasets">Towards affordable large-scale autonomous driving datasets</a> </div> <ul> <li> <a href="#ingredient-1-unsupervised-object-discovery-and-self-supervised-learning">Ingredient 1: Unsupervised object discovery and self-supervised learning</a> </li> <li> <a href="#ingredient-2-active-learning">Ingredient 2: Active learning</a> </li> <li> <a href="#ingredient-3-foundation-and-language-models">Ingredient 3: Foundation and Language Models</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <p>Recent advances in computer vision and artificial intelligence paved the way for major leaps in machine perception and understanding–key elements of autonomous systems. As a result, autonomous vehicles (AVs), particularly self-driving cars, have risen in popularity in the past several years. Tesla is arguably the most well-known company in the AV space. Autonomous taxis–or robotaxis–are also gaining traction. Waymo (formerly the Google Self-Driving Car Project) and Motional (formerly nuTonomy) are two of the several companies competing in the AV industry.</p> <h2 id="the-case-for-autonomous-vehicles">The case for autonomous vehicles</h2> <p>Enhanced safety is one of the primary motivations for developing AV technology.</p> <p>In autonomous vehicles, the degree of autonomy is <a href="https://www.sae.org/standards/content/j3016_202104/" rel="external nofollow noopener" target="_blank">classified into 6 levels</a>. Level 0 pertains to the case where a human is 100% in control of the vehicle and no automation is employed. Meanwhile, the other end of the spectrum is Level 5–the fully autonomous case–where a human is merely a passenger in a driverless car. As of this writing, no system has achieved Level 5 yet.</p> <p>Waymo was the <a href="https://www.forbes.com/sites/alanohnsman/2024/01/08/waymos-robotaxis-are-hitting-the-highway-a-first-for-self-driving-cars" rel="external nofollow noopener" target="_blank">first to achieve Level 4</a> autonomy, or <em>high driving automation</em>. At Level 4, a vehicle is autonomous only for certain specific conditions or geographical areas, e.g. driving in the freeway, or backing out of a garage. In contrast, Level 5 AVs are fully autonomous 100% of the time, and should be able to seamlessly cooperate with each other, enhancing passenger safety and reducing–if not totally eliminating–human-caused accidents.</p> <h2 id="road-to-full-autonomy">Road to full autonomy</h2> <figure> <iframe src="https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2Fupeeei%2Fvideos%2F408094458765889" class="img-fluid rounded z-depth-1 responsive-iframe" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> <figcaption class="caption"><strong>Figure 1</strong>. Advancing AI: A Fireside Chat &amp; Research Talk with Dr. Holger Caesar</figcaption> </figure> <p>How do we progress from the current state-of-the-art, Level 4, to the next frontier–Level 5? This is one of the key problems being tackled by <a href="https://sites.google.com/it-caesar.de/homepage/" rel="external nofollow noopener" target="_blank">Dr. Holger Caesar</a>, Assistant Professor at TU Delft and former Principal Research Scientist at Motional. In his talk at the Electrical and Electronics Engineering Institute in UP Diliman last May 9, 2024 (Figure 1), he highlighted the important correlation between the number of human interventions per mile driven and training data quantity, as shown in Figure 2. Interventions per mile driven is <a href="https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/disengagement-reports/" rel="external nofollow noopener" target="_blank">used by the California Department of Motor Vehicles</a> as a proxy measure for AV safety, where lower number of interventions mean safer autonomous vehicles. This measure is considered the industry standard.</p> <pre><code class="language-echarts">{
  "tooltip": {
    "trigger": "axis"
  },
  "xAxis": {
    "type": "log",
    "name": "Training dataset size (hours)",
    "nameLocation": "middle",
    "nameGap": 20,
    "max": 2000
  },
  "yAxis": {
    "name": "Interventions per 1,000 miles",
    "nameLocation": "middle",
    "nameGap": 40
  },
  "series": [
    {
      "data": [[3.5, 2190.7], [10, 1024.1], [100, 145.3], [1000, 93.9]],
      "type": "line",
      "label": {
        "show": true
      }
    }
  ]
}
</code></pre> <div class="caption"> <strong>Figure 2.</strong> Human interventions per 1,000 miles vs training dataset size from Houston et al. <d-cite key="houston2021one"></d-cite>. </div> <p>While larger AV datasets are desirable, the costs associated in acquiring them are not. Data acquisition costs increase with the amount of data collected and annotated. Manually annotating vast amounts of data is prohibitively expensive. As Dr. Caesar remarked, only billion-dollar companies could afford to annotate such large-scale datasets. What is the solution then?</p> <h2 id="towards-affordable-large-scale-autonomous-driving-datasets">Towards affordable large-scale autonomous driving datasets</h2> <p>Due to significant differences in traffic rules, infrastructure, and road user mix across countries, or even cities, a dataset is inherently tied to the place where it is acquired. Therefore, city-specific datasets are preferred since models trained on these will always outperform generic ones.</p> <p>Aside from the technical aspects, there are human factors involved too. In the Philippines, for example, <a href="https://www.business-humanrights.org/en/latest-news/philippines-scale-ai-creating-race-to-the-bottom-as-outsourced-workers-face-poor-conditions-in-digital-sweatshops-incl-low-wages-withheld-payments/" rel="external nofollow noopener" target="_blank">workers often face “digital sweatshop” conditions</a> because the data annotation industry is still considered an informal sector and is unregulated.</p> <p>Dr. Caesar highlighted the need for affordable dataset acquisition in order to democratize autonomous vehicle research. Without an affordable way to acquire data, countries lacking in resources will be left behind, and human annotators will continue to be exploited. The following sections discuss the three key <em>ingredients</em> of the affordable auto-labeling framework he proposed.</p> <h3 id="ingredient-1-unsupervised-object-discovery-and-self-supervised-learning">Ingredient 1: Unsupervised object discovery and self-supervised learning</h3> <p>Unsupervised and self-supervised learning are learning paradigms which take advantage of vast amounts of unlabeled data. While the two terms are oftentimes used interchangeably in the literature, there is a nuanced difference between them <d-cite key="lecun2021ssl"></d-cite>. Simply put, self-supervised learning paradigms have a common explicit objective: to reconstruct the whole given some of its parts. The intuition is that this objective coerces the model to learn patterns in the data, not memorize it.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dao_2024_label-efficient-3d-object-detection-rsu-480.webp 480w,/assets/img/dao_2024_label-efficient-3d-object-detection-rsu-800.webp 800w,/assets/img/dao_2024_label-efficient-3d-object-detection-rsu-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dao_2024_label-efficient-3d-object-detection-rsu.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><strong>Figure 3.</strong> Overview of the 3D object detection method of Dao et al. Extracted from the paper.</figcaption> </figure> <p>Dao et al. <d-cite key="dao2024label"></d-cite>–to be presented at the <em>35th IEEE Intelligent Vehicles Symposium</em>–is a recent paper by Dr. Caesar and his collaborators. In this work, they introduce an unsupervised object discovery method, as well as adapt the self-training method of Xie et al. <d-cite key="xie2020self"></d-cite> for object detection.</p> <p>Figure 3 illustrates the overview of their method. Self-training is done as follows:</p> <ol> <li>The output of the unsupervised object discovery method serves as the initial labeled training data.</li> <li>An initial detection model is trained on this initial dataset.</li> <li>Once trained, the detection model is then used on the training set to detect objects. Only the high-confidence detections are selected, and these are treated as ground-truth. In the literature, these model-generated labels being used as ground-truth are called pseudo-labels.</li> <li>The pseudo-labels are then used to train a new detection model, and the cycle is repeated (steps 3 and 4) until a satisfactory level of performance is achieved.</li> </ol> <p>Using their proposed method, the <strong>same performance</strong> can be achieved with <strong>2-5x less manual annotation</strong> effort.</p> <h3 id="ingredient-2-active-learning">Ingredient 2: Active learning</h3> <p>In machine learning, training on more data is generally better. At some point however, the cost of annotating a data sample exceeds the benefits of being able to use that labeled sample for training. Model performance will start to saturate at some point, and using more data beyond that no longer makes sense. Furthermore, the resources available–money–limit the amount of data which can be acquired. Therefore, machine learning practitioners should be clever and efficient in choosing training samples. In other words, the goal is to minimize the training data required to meet the desired model performance.</p> <p>Large datasets are bound to have redundancies. Active learning is a learning paradigm which focuses on the selection of the <em>most informative</em> representative samples from a large pool of (possibly unlabeled) data. <a href="https://jacobgil.github.io/deeplearning/activelearning" rel="external nofollow noopener" target="_blank">Jacob Gildenblat’s article</a> provides a more comprehensive overview of active learning in the context of deep learning.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tan_2023_active-learning-480.webp 480w,/assets/img/tan_2023_active-learning-800.webp 800w,/assets/img/tan_2023_active-learning-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tan_2023_active-learning.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><strong>Figure 4.</strong> Overview of cross-modal active learning by Tan et al. Extracted from Dr. Caesar's presentation.</figcaption> </figure> <p>The patent of Tan et al. <d-cite key="tan2024cross"></d-cite> is illustrated in Figure 4. It claims a method for applying active learning to object detection by comparing the consistency of results from two different modalities–images and LiDAR data. Samples where the detection results are consistent across the two modalities are considered high-confidence predictions, and can be used as-is for training detection models. On the other hand, samples which produce inconsistent detections are most likely challenging objects and would require manual annotation.</p> <h3 id="ingredient-3-foundation-and-language-models">Ingredient 3: Foundation and Language Models</h3> <p>Training a large model from scratch requires massive datasets and tremendous compute power. However, not everyone has the resources to do this. To lower the data requirement and shorten the training time, an initial condition close to a local optima is crucial. When two tasks are similar enough, existing trained weights can be reused instead of resorting to random initialization. This is the key idea behind transfer learning and finetuning.</p> <p>The introduction of the Transformer <d-cite key="vaswani2017attention"></d-cite> in 2017 ushered in new breakthroughs in the realm of natural language processing (NLP). Language models such as BERT <d-cite key="devlin-etal-2019-bert"></d-cite> and GPT <d-cite key="radford2018improving"></d-cite> were pretrained first on large unlabeled text corpora, then finetuned to specific downstream tasks using labeled data. The effectiveness of their approach made the pretraining + finetuning paradigm a staple in NLP research. In the vision domain, pretrained models such as DINO <d-cite key="caron2021emerging"></d-cite> soon followed and played a similar role in computer vision research. These large pretrained models are collectively known today as foundation models <d-cite key="bommasani2021opportunities"></d-cite>. From a software engineering perspective, foundation models are akin to the standard libraries of programming languages. They provide a solid base where countless downstream models can be finetuned from. In the grand scheme of things, foundation models reduce the overall cost of model training.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/oquab_2024_dino-v2-480.webp 480w,/assets/img/oquab_2024_dino-v2-800.webp 800w,/assets/img/oquab_2024_dino-v2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/oquab_2024_dino-v2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><strong>Figure 5.</strong> Visualization of the first PCA components from DINO v2. Extracted from the paper.</figcaption> </figure> <p>Desirable properties–such as understanding of object parts–typically emerge in self-supervised vision models like DINO v2 <d-cite key="oquab2023dinov2"></d-cite>. In Figure 5, notice that functionally similar object parts from different classes, e.g. the wings and nose of a plane and of an eagle, are matched and depicted with the same colors. In an ongoing work, Dr. Caesar and his colleagues exploit these emergent properties in the task of unsupervised 3D object detection.</p> <h2 id="conclusion">Conclusion</h2> <p>Safety is one of the driving motivations behind autonomous vehicle research. By eliminating the possibility of human error, autonomous vehicles are touted as safer alternatives. However, we are still far from achieving fully autonomous vehicles which could operate reliably in diverse and unpredictable environments. One of the major roadblocks is dataset acquisition. Autonomous vehicles would require vast amounts of annotated data in order to reach a full degree of autonomy.</p> <p>Large-scale manually annotated data is not economically viable, even for big well-funded companies. Thus, we have to be clever about data acquisition while simultaneously developing label-efficient training methods. Dr. Caesar presented a framework for democratizing autonomous vehicle research by making dataset annotation more affordable. By using unsupervised and self-supervised methods, active learning, and foundation models, auto-labeling can be made cheap and effective. Future work should tackle how to use these three ingredients in synergy.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-05-17-datasets-for-autonomous-vehicles.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Darwin M. Bautista. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 26, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script>let theme=determineComputedTheme();document.onreadystatechange=(()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-echarts").forEach(e=>{const t=e.textContent,a=e.parentElement;a.classList.add("unloaded");let n=document.createElement("div");if(n.classList.add("echarts"),a.after(n),"dark"===theme)var r=echarts.init(n,"dark-fresh-cut");else r=echarts.init(n);r.setOption(JSON.parse(t)),window.addEventListener("resize",function(){r.resize()})})});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>